{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ad961b-bcd1-4283-b8de-f18b56ba9280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ í™œì„±í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ ì„¤ì •\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ í™œì„±í™” ì™„ë£Œ\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b39f21-f2af-4ca0-a4e4-5be82a6aa9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/tf_metal/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV íŒŒì¼ ê°œìˆ˜: 25456\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. ë°ì´í„° ì••ì¶• í•´ì œ ë° í´ë¦¬ë‹\n",
    "extract_dir = \"ê³µê°í˜•ë™ì ëŒ€í™”(ì••ì¶•í‘¼ê²ƒ)/train\"\n",
    "cleaned_dir = \"./cleaned_data\"\n",
    "checkpoint_file = os.path.join(cleaned_dir, \"cleaned_files.json\")\n",
    "\n",
    "if not os.path.exists(cleaned_dir):\n",
    "    os.makedirs(cleaned_dir)\n",
    "\n",
    "# TSV íŒŒì¼ íƒìƒ‰\n",
    "tsv_files = glob.glob(os.path.join(extract_dir, \"**/*.tsv\"), recursive=True)\n",
    "print(f\"TSV íŒŒì¼ ê°œìˆ˜: {len(tsv_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d2d512f-d499-498c-b7b5-0cebb40da0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ í´ë¦¬ë‹ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë˜ëŠ” í´ë¦¬ë‹ ìˆ˜í–‰\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cleaned_tsv_files = json.load(f)\n",
    "else:\n",
    "    cleaned_tsv_files = []\n",
    "    for tsv_file in tsv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(tsv_file, delimiter=\"\\t\", on_bad_lines=\"skip\", engine=\"python\")\n",
    "            if \"utterance_text\" in df.columns:\n",
    "                df[\"utterance_text\"] = df[\"utterance_text\"].astype(str).apply(clean_text)\n",
    "                cleaned_file = os.path.join(cleaned_dir, os.path.basename(tsv_file))\n",
    "                df.to_csv(cleaned_file, sep=\"\\t\", index=False)\n",
    "                cleaned_tsv_files.append(cleaned_file)\n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë¥˜ ë°œìƒ: {tsv_file}: {e}\")\n",
    "    with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_tsv_files, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb057954-5168-44ff-867b-98177e71ca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ìˆ˜ì§‘ëœ ë¬¸ì¥ ìˆ˜: 378562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "2025-04-17 01:27:45.259358: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-04-17 01:27:45.259672: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-04-17 01:27:45.259704: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744820865.260565 21025700 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744820865.260604 21025700 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.4.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tensor ì¤€ë¹„ ì™„ë£Œ: (378562, 128) (378562, 128) (378562, 128)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "texts = []\n",
    "for file in cleaned_tsv_files:\n",
    "    df = pd.read_csv(file, sep=\"\\t\")\n",
    "    if \"utterance_text\" in df.columns:\n",
    "        texts.extend(df[\"utterance_text\"].dropna().tolist())\n",
    "\n",
    "print(f\"ì´ ìˆ˜ì§‘ëœ ë¬¸ì¥ ìˆ˜: {len(texts)}\")\n",
    "\n",
    "# 3. KoGPT2 í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (âœ… ì˜¬ë°”ë¥¸ FastTokenizer ì‚¬ìš©)\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    model_name,\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name, from_pt=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4. ë°ì´í„° í† í¬ë‚˜ì´ì§•\n",
    "def encode_sentences(sentences, max_length=128):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for text in sentences:\n",
    "        enc = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids.append(enc[\"input_ids\"])\n",
    "        attention_masks.append(enc[\"attention_mask\"])\n",
    "        labels.append(enc[\"input_ids\"])  # GPT-2ëŠ” ì…ë ¥=ì¶œë ¥\n",
    "\n",
    "    return (\n",
    "        tf.concat(input_ids, axis=0),\n",
    "        tf.concat(attention_masks, axis=0),\n",
    "        tf.concat(labels, axis=0)\n",
    "    )\n",
    "\n",
    "# 5. ë¬¸ì¥ ë¶ˆëŸ¬ì˜¤ê¸° í›„ ì¸ì½”ë”© (ì˜ˆ: cleaned_tsv_files ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸)\n",
    "X, A, y = encode_sentences(texts)\n",
    "print(\"âœ… Tensor ì¤€ë¹„ ì™„ë£Œ:\", X.shape, A.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695fcf89-e8e3-4d45-af67-2cd78912a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378562, 128) <dtype: 'int32'>\n",
      "(378562, 128) <dtype: 'int32'>\n",
      "(378562, 128) <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, X.dtype)  # ex: (378562, 128), tf.int32\n",
    "print(A.shape, A.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a94626-6e22-4f77-aa9d-1ede4a3416a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 01:34:04.685811: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744821245.252467 21025700 meta_optimizer.cc:967] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94641/94641 [==============================] - 14167s 150ms/step - loss: 0.0011\n",
      "Epoch 2/3\n",
      "94641/94641 [==============================] - 13987s 148ms/step - loss: 6.9952e-05\n",
      "Epoch 3/3\n",
      "94641/94641 [==============================] - 14262s 151ms/step - loss: 3.2567e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./kogpt2-finetuned-final/tokenizer_config.json',\n",
       " './kogpt2-finetuned-final/special_tokens_map.json',\n",
       " './kogpt2-finetuned-final/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pad í† í° ì„¤ì •\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 6. ëª¨ë¸ ì»´íŒŒì¼\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# 7. í•™ìŠµ ìˆ˜í–‰\n",
    "model.fit(\n",
    "    {\"input_ids\": X, \"attention_mask\": A},\n",
    "    y,\n",
    "    batch_size=4,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# 8. ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./kogpt2-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./kogpt2-finetuned-final\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c08b77-42c3-4092-9694-14d70a667b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.4.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¾ ìƒì„± ê²°ê³¼:\n",
      " ì˜¤ëŠ˜ì€ ë„ˆë¬´ íŒ”ì´ ì•„íŒŒì„œ ë³‘ì›ì— ê°”ë”ë‹ˆ ê·¸ê²Œ ì•ˆ ë˜ë”ë¼ê³ ìš”.\n",
      "ê·¸ë˜ ê°€ì§€ê³  ì œê°€ ê·¸ë•ŒëŠ” ê·¸ëƒ¥ ì €í•œí…Œ ì´ë ‡ê²Œ ì–˜ê¸°í–ˆì—ˆì–´ìš”.\n",
      "ê·¸ë¬ëŠ”ë° ì´ì œ ì´ê±° ì–´ë–»ê²Œ í•´ì•¼ ë ì§€ ëª¨ë¥´ê² ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\", from_pt=True)\n",
    "\n",
    "# 2. í…ìŠ¤íŠ¸ ì…ë ¥\n",
    "text = \"ì˜¤ëŠ˜ì€ ë„ˆë¬´ íŒ”ì´ ì•„íŒŒì„œ\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "attention_mask = tf.ones_like(input_ids)\n",
    "\n",
    "# 3. í…ìŠ¤íŠ¸ ìƒì„±\n",
    "gen_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=64,\n",
    "    repetition_penalty=2.0,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# 4. ë””ì½”ë”© ë° í›„ì²˜ë¦¬\n",
    "generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 5. ë§ˆì¹¨í‘œ(.)ë¡œ ëë‚˜ì§€ ì•ŠëŠ” ë§ˆì§€ë§‰ ë¬¸ì¥ ì œê±°\n",
    "def remove_unfinished_sentence(text):\n",
    "    sentences = text.strip().split('\\n')\n",
    "    if not sentences[-1].strip().endswith('.'):\n",
    "        sentences = sentences[:-1]\n",
    "    return '\\n'.join(sentences)\n",
    "\n",
    "cleaned_output = remove_unfinished_sentence(generated)\n",
    "\n",
    "# 6. ì¶œë ¥\n",
    "print(\"ğŸ§¾ ìƒì„± ê²°ê³¼:\\n\", cleaned_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_metal)",
   "language": "python",
   "name": "tf_metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
