{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ad961b-bcd1-4283-b8de-f18b56ba9280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 동적 할당 활성화 완료\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# GPU 메모리 동적 할당 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU 메모리 동적 할당 활성화 완료\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b39f21-f2af-4ca0-a4e4-5be82a6aa9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/tf_metal/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV 파일 개수: 25456\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. 데이터 압축 해제 및 클리닝\n",
    "extract_dir = \"공감형동적대화(압축푼것)/train\"\n",
    "cleaned_dir = \"./cleaned_data\"\n",
    "checkpoint_file = os.path.join(cleaned_dir, \"cleaned_files.json\")\n",
    "\n",
    "if not os.path.exists(cleaned_dir):\n",
    "    os.makedirs(cleaned_dir)\n",
    "\n",
    "# TSV 파일 탐색\n",
    "tsv_files = glob.glob(os.path.join(extract_dir, \"**/*.tsv\"), recursive=True)\n",
    "print(f\"TSV 파일 개수: {len(tsv_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d2d512f-d499-498c-b7b5-0cebb40da0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 클리닝 함수\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# 체크포인트 로드 또는 클리닝 수행\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cleaned_tsv_files = json.load(f)\n",
    "else:\n",
    "    cleaned_tsv_files = []\n",
    "    for tsv_file in tsv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(tsv_file, delimiter=\"\\t\", on_bad_lines=\"skip\", engine=\"python\")\n",
    "            if \"utterance_text\" in df.columns:\n",
    "                df[\"utterance_text\"] = df[\"utterance_text\"].astype(str).apply(clean_text)\n",
    "                cleaned_file = os.path.join(cleaned_dir, os.path.basename(tsv_file))\n",
    "                df.to_csv(cleaned_file, sep=\"\\t\", index=False)\n",
    "                cleaned_tsv_files.append(cleaned_file)\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {tsv_file}: {e}\")\n",
    "    with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_tsv_files, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb057954-5168-44ff-867b-98177e71ca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 수집된 문장 수: 378562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "2025-04-17 01:27:45.259358: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-04-17 01:27:45.259672: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-04-17 01:27:45.259704: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744820865.260565 21025700 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744820865.260604 21025700 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.4.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tensor 준비 완료: (378562, 128) (378562, 128) (378562, 128)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "# 2. 데이터 로드\n",
    "texts = []\n",
    "for file in cleaned_tsv_files:\n",
    "    df = pd.read_csv(file, sep=\"\\t\")\n",
    "    if \"utterance_text\" in df.columns:\n",
    "        texts.extend(df[\"utterance_text\"].dropna().tolist())\n",
    "\n",
    "print(f\"총 수집된 문장 수: {len(texts)}\")\n",
    "\n",
    "# 3. KoGPT2 토크나이저 및 모델 로드 (✅ 올바른 FastTokenizer 사용)\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    model_name,\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name, from_pt=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4. 데이터 토크나이징\n",
    "def encode_sentences(sentences, max_length=128):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for text in sentences:\n",
    "        enc = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        input_ids.append(enc[\"input_ids\"])\n",
    "        attention_masks.append(enc[\"attention_mask\"])\n",
    "        labels.append(enc[\"input_ids\"])  # GPT-2는 입력=출력\n",
    "\n",
    "    return (\n",
    "        tf.concat(input_ids, axis=0),\n",
    "        tf.concat(attention_masks, axis=0),\n",
    "        tf.concat(labels, axis=0)\n",
    "    )\n",
    "\n",
    "# 5. 문장 불러오기 후 인코딩 (예: cleaned_tsv_files 기반 텍스트 리스트)\n",
    "X, A, y = encode_sentences(texts)\n",
    "print(\"✅ Tensor 준비 완료:\", X.shape, A.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695fcf89-e8e3-4d45-af67-2cd78912a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378562, 128) <dtype: 'int32'>\n",
      "(378562, 128) <dtype: 'int32'>\n",
      "(378562, 128) <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, X.dtype)  # ex: (378562, 128), tf.int32\n",
    "print(A.shape, A.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a94626-6e22-4f77-aa9d-1ede4a3416a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 01:34:04.685811: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744821245.252467 21025700 meta_optimizer.cc:967] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94641/94641 [==============================] - 14167s 150ms/step - loss: 0.0011\n",
      "Epoch 2/3\n",
      "94641/94641 [==============================] - 13987s 148ms/step - loss: 6.9952e-05\n",
      "Epoch 3/3\n",
      "94641/94641 [==============================] - 14262s 151ms/step - loss: 3.2567e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./kogpt2-finetuned-final/tokenizer_config.json',\n",
       " './kogpt2-finetuned-final/special_tokens_map.json',\n",
       " './kogpt2-finetuned-final/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pad 토큰 설정\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 6. 모델 컴파일\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# 7. 학습 수행\n",
    "model.fit(\n",
    "    {\"input_ids\": X, \"attention_mask\": A},\n",
    "    y,\n",
    "    batch_size=4,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# 8. 모델 저장\n",
    "model.save_pretrained(\"./kogpt2-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./kogpt2-finetuned-final\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c08b77-42c3-4092-9694-14d70a667b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.4.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 생성 결과:\n",
      " 오늘은 너무 팔이 아파서 병원에 갔더니 그게 안 되더라고요.\n",
      "그래 가지고 제가 그때는 그냥 저한테 이렇게 얘기했었어요.\n",
      "그랬는데 이제 이거 어떻게 해야 될지 모르겠다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. 토크나이저 및 모델 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\", from_pt=True)\n",
    "\n",
    "# 2. 텍스트 입력\n",
    "text = \"오늘은 너무 팔이 아파서\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "attention_mask = tf.ones_like(input_ids)\n",
    "\n",
    "# 3. 텍스트 생성\n",
    "gen_ids = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=64,\n",
    "    repetition_penalty=2.0,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# 4. 디코딩 및 후처리\n",
    "generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 5. 마침표(.)로 끝나지 않는 마지막 문장 제거\n",
    "def remove_unfinished_sentence(text):\n",
    "    sentences = text.strip().split('\\n')\n",
    "    if not sentences[-1].strip().endswith('.'):\n",
    "        sentences = sentences[:-1]\n",
    "    return '\\n'.join(sentences)\n",
    "\n",
    "cleaned_output = remove_unfinished_sentence(generated)\n",
    "\n",
    "# 6. 출력\n",
    "print(\"🧾 생성 결과:\\n\", cleaned_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_metal)",
   "language": "python",
   "name": "tf_metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
